import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

from sklearn.feature_extraction import DictVectorizer
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix,classification_report
from sklearn.model_selection import KFold
from sklearn.metrics import roc_curve, roc_auc_score , auc


def Colorful_Confusion_Matrix(y, y_test, y_pred):
    cm=confusion_matrix(y_test, y_pred)
    l=len(set(y))
    df_cm = pd.DataFrame(cm, range(l), range(l))
    ax=sns.heatmap(df_cm,cmap="Blues", annot=True,annot_kws={"size": 16})
    bottom, top = ax.get_ylim()
    ax.set_ylim(bottom + 0.5, top - 0.5)
    plt.ylabel('True label');
    plt.xlabel('Predicted label');
    plt.title("Confusion Matrix", size = 16)
    plt.savefig('CM_Test_01.png')
    


def plot_roc_curve(fpr, tpr,AUC):  
    plt.plot(fpr, tpr, color='orange', label='ROC')
    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve with AUC {:.3f}'.format(AUC))
    plt.legend()
    plt.show()
    plt.savefig('roc.png')
    
def plot_pr(recall,precision):
    auc_score = auc(recall, precision)
    plt.plot([0, 1], [0.5, 0.5], linestyle='--')
    plt.plot(recall, precision, marker='.')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Recall vs Precision Curve with AUC {:.3f}'.format(auc_score))
    plt.savefig('PR.png')
    plt.show()
    
    print('AUC: %.3f' % auc_score)
    
def cross_validation_accuracy(clf,X,y,cv):
    scores = cross_val_score(clf, X, y, cv=cv)
    print(scores)
    print('Accuracy of : {:.3f} Â± {:.3f}'.format(np.mean(scores), 2 * np.std(scores)))
